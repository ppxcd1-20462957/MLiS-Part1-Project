from sklearn.datasets import fetch_california_housing

#imported random dataset, just change X,y here when inputting new dataset
cali = fetch_california_housing()

X = cali.data
y = cali.target

print(X.shape)
print(y.shape)

print(cali.feature_names)

#done just to give heading to every column in dataset, ignore in arctic dataset
import pandas as pd
df = pd.DataFrame(X)
df.columns = cali.feature_names
df.head()

#Z normalisation, axis 0 means along x axis so it takes value of each row for one column. my dataset here had 8 columns so it gave 8 mean and std for each column.
import numpy as np
u = np.mean(X,axis=0)
std = np.std(X,axis=0)
print(u.shape, std.shape)

X = (X-u)/std
df = pd.DataFrame(X)
df.columns = cali.feature_names
df.head()

#adding the column of one for bias
ones = np.ones((X.shape[0],1))
X = np.hstack((ones,X))
print(X.shape)
df = pd.DataFrame(X)
df.head()




# X - matrix (m X n)
# x - vector (single example with n features)

def hypothesis(x,theta):
    y_hat = 0.0
    n = x.shape[0]
    for i in range(n):
        y_hat += (theta[i]*x[i])
    return y_hat

def error(X,Y,theta):
    e = 0.0
    m = X.shape[0]
    
    for i in range(m):
        y_hat = hypothesis(X[i],theta)
        e += (y[i]-y_hat)**2
    return e/m
    
def gradient(X,y,theta):
    m,n = X.shape
    grad = np.zeros((n,))
    
    #for all values of j
    for j in range(n):
        #sum over all examples
        for i in range(m):
            y_hat = hypothesis(X[i],theta)
            grad[j] += (y_hat - y[i])*X[i][j]
    return grad/m

def gradient_descent(X,y,learning_rate=0.1,max_epochs=300):
    m,n = X.shape
    theta = np.zeros((n,))
    error_list = []
    
    for i in range(max_epochs):
        e = error(X,y,theta)
        error_list.append(e)
        
        grad = gradient(X,y,theta)
        for j in range(n):
            theta[j] = theta[j] - learning_rate*grad[j]
    return theta,error_list


theta, error_list = gradient_descent(X,y)
print(theta)
import matplotlib.pyplot as plt
plt.plot(error_list)
plt.show()

#calculating predicted values to calculate R2 score

y_hat = []

m=X.shape[0]

for i in range(m):
    pred = hypothesis(X[i],theta)
    y_hat.append(pred)
    
y_hat = np.array(y_hat)

print(y_hat)

def r2_score(y,y_hat):
    num = np.sum((y-y_hat)**2)
    denom = np.sum((y-y.mean())**2)
    score = (1-num/denom)
    return score*100

r2_score(y,y_hat)
